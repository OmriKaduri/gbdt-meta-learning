{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "from catboost import Pool, cv, CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "import glob\n",
    "from sklearn.model_selection import cross_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def fpr_tpr(y_true, y_pred):\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "\n",
    "    fpr = fp / (fp + tn)\n",
    "    tpr = tp / (tp + fn) #Precision\n",
    "    \n",
    "    return fpr, tpr\n",
    "\n",
    "\n",
    "def fpr_tpr_for_threshold(y_true, y_pred_probs, threshold=.5):\n",
    "    n_classes = np.unique(y_true)\n",
    "    num_paires = (len(n_classes) * (len(n_classes) - 1)) // 2\n",
    "    fpr_scores = np.zeros(num_paires)\n",
    "    tpr_scores = np.zeros(num_paires)\n",
    "    for ix, (a, b) in enumerate(combinations(n_classes, 2)): # one vs one\n",
    "        a_mask = y_true == a\n",
    "        b_mask = y_true == b\n",
    "        ab_mask = np.logical_or(a_mask, b_mask)\n",
    "\n",
    "        a_true = a_mask[ab_mask]\n",
    "        b_true = b_mask[ab_mask]\n",
    "        y_pred_a = np.where(y_pred_probs[ab_mask, a] >= threshold, 1, 0)\n",
    "        y_pred_b = np.where(y_pred_probs[ab_mask, b] >= threshold, 1, 0)\n",
    "\n",
    "        fpr_a, tpr_a = fpr_tpr(a_true, y_pred_a)\n",
    "        fpr_b, tpr_b = fpr_tpr(b_true, y_pred_b)\n",
    "        fpr_scores[ix] = (fpr_a + fpr_b) / 2\n",
    "        tpr_scores[ix] = (tpr_a + tpr_b) / 2\n",
    "        \n",
    "    return np.average(fpr_scores), np.average(tpr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_precision_recall_curve(y_true, y_pred_probs):\n",
    "    n_classes = np.unique(y_true)\n",
    "    num_paires = (len(n_classes) * (len(n_classes) - 1)) // 2\n",
    "    ap_scores = np.zeros(num_paires)\n",
    "    \n",
    "    for ix, (a, b) in enumerate(combinations(n_classes, 2)): # one vs one\n",
    "        a_mask = y_true == a\n",
    "        b_mask = y_true == b\n",
    "        ab_mask = np.logical_or(a_mask, b_mask)\n",
    "\n",
    "        a_true = a_mask[ab_mask]\n",
    "        b_true = b_mask[ab_mask]\n",
    "        \n",
    "        a_ap = average_precision_score(a_true, y_pred_probs[ab_mask, a])\n",
    "        b_ap = average_precision_score(b_true, y_pred_probs[ab_mask, b])\n",
    "        ap_scores[ix] = (a_ap + b_ap) / 2\n",
    "        \n",
    "    return np.average(ap_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, precision_score, roc_auc_score, roc_curve\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "class HyperoptObjective(object):\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, model, const_params, fit_params):\n",
    "        self.evaluated_count = 0\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.model = model\n",
    "        self.constant_params = const_params\n",
    "        self.fit_params = fit_params\n",
    "        if self.y_train.dtype == 'object':\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            self.y_train = le.fit_transform(self.y_train)\n",
    "            self.y_test = le.fit_transform(self.y_test)\n",
    "            \n",
    "        \n",
    "    '''\n",
    "    The way that HyperOpt fmin function works, is that on each evaluation \n",
    "    it calls given objective function. \n",
    "    Since we decided to declare our objective as class instead of a function,\n",
    "    we will implement the evaluation logic inside the __call__ method.\n",
    "    '''\n",
    "    def __call__(self, hyper_params):\n",
    "        \n",
    "        model = self.model(**hyper_params, **self.constant_params)\n",
    "        fit_start = timer()\n",
    "        best = model.fit(X=self.X_train,y=self.y_train,**self.fit_params)\n",
    "        fit_end = timer()\n",
    "        fit_time = fit_end - fit_start\n",
    "        infer_start = timer()\n",
    "        test_preds_proba = best.predict_proba(self.X_test)\n",
    "        test_preds = np.argmax(test_preds_proba, axis=1)\n",
    "        infer_end = timer()\n",
    "        infer_time = infer_end - infer_start\n",
    "\n",
    "        self.evaluated_count += 1\n",
    "        fpr, tpr = fpr_tpr_for_threshold(self.y_test, test_preds_proba)\n",
    "        \n",
    "        is_multiclass = len(np.unique(self.y_train)) > 2\n",
    "        if is_multiclass:\n",
    "            auc = roc_auc_score(self.y_test,test_preds_proba,multi_class='ovo', average='macro') \n",
    "            precision = precision_score(self.y_test, test_preds, average='macro')\n",
    "            pr_curve = multiclass_precision_recall_curve(self.y_test, test_preds_proba) # Macro avg\n",
    "        else:\n",
    "            test_preds_proba = np.max(test_preds_proba,axis=1)\n",
    "            auc = roc_auc_score(self.y_test,test_preds_proba)\n",
    "            precision = precision_score(self.y_test, test_preds)\n",
    "            pr_curve = average_precision_score(self.y_test, test_preds_proba)\n",
    "            \n",
    "        return {'loss':-accuracy_score(self.y_test, test_preds),\n",
    "                'status':STATUS_OK,\n",
    "                'metrics':{\n",
    "                    'Accuracy':accuracy_score(self.y_test, test_preds),\n",
    "                    'TPR':tpr, #macro average\n",
    "                    'FPR':fpr, #macro average\n",
    "                    'Precision': precision,\n",
    "                    'AUC': auc,\n",
    "                    'PR-Curve': pr_curve,\n",
    "                    'Training_time':fit_time,\n",
    "                    'Inference_time':infer_time\n",
    "                    }\n",
    "                }\n",
    "                #NOTE: The negative sign is due to that fact that we optimize for accuracy,\n",
    "                          #therefore we want to minimize the negative acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params(X_train, \n",
    "                     y_train,   \n",
    "                     X_test,\n",
    "                     y_test,\n",
    "                     model,\n",
    "                     const_params, \n",
    "                     parameter_space, \n",
    "                     fit_params={},\n",
    "                     max_evals=25,\n",
    "                     cv_splitter=None,\n",
    "                     cv_scoring=None\n",
    "                    ):\n",
    "    \n",
    "    objective = HyperoptObjective(X_train, y_train, X_test, y_test, model, const_params, \n",
    "                                  fit_params)\n",
    "    '''\n",
    "    HyperOpt Trials object stores details of every iteration. \n",
    "    https://github.com/hyperopt/hyperopt/wiki/FMin#12-attaching-extra-information-via-the-trials-object\n",
    "    '''\n",
    "    trials = hyperopt.Trials()\n",
    "    \n",
    "    '''\n",
    "    Hyperopt fmin function returns only parameters from the search space.\n",
    "    Therefore, before returning best_params\n",
    "    we will merge best_params with the const params, \n",
    "    so we have all parameters in one place for training the best model.\n",
    "    '''\n",
    "    best_params = hyperopt.fmin(\n",
    "        fn=objective,\n",
    "        space=parameter_space,\n",
    "        algo=hyperopt.tpe.suggest,\n",
    "        rstate=np.random.RandomState(seed=42),\n",
    "        max_evals=max_evals,\n",
    "        trials=trials\n",
    "    )\n",
    "      \n",
    "    best_params.update(const_params)\n",
    "    \n",
    "    return best_params, trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "def inner_cv_hyperopt(X,y, n_splits=3):\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    const_params = {\n",
    "        'verbose': False\n",
    "    }\n",
    "    parameter_space = {\n",
    "        'n_estimators': hyperopt.hp.choice('n_estimators', np.arange(50, 250, 25)),\n",
    "        'max_depth': hyperopt.hp.choice('max_depth', np.arange(5, 9)),\n",
    "        'learning_rate': hyperopt.hp.uniform('learning_rate', 0.01, 0.5),\n",
    "    }\n",
    "    best_acc = 0\n",
    "    best_auc = 0\n",
    "    best_precision = 0\n",
    "    for index, (tr_ind, test_ind) in enumerate(kf.split(X,y)):\n",
    "        print(\"Starting {i} fold out of {n} inner folds\".format(i=index,n=n_splits))\n",
    "        \n",
    "        X_train = X.iloc[tr_ind].copy()\n",
    "        y_train = y.iloc[tr_ind].copy()\n",
    "        \n",
    "        X_test = X.iloc[test_ind].copy()\n",
    "        y_test = y.iloc[test_ind].copy()\n",
    "        cat_features = X_train.select_dtypes(include=['object','category']).columns\n",
    "        fit_params = {\n",
    "            'cat_features': cat_features\n",
    "        }\n",
    "\n",
    "        curr_best_params, trials = find_best_params(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            X_test,\n",
    "            y_test,\n",
    "            CatBoostClassifier,\n",
    "            const_params,\n",
    "            parameter_space,\n",
    "            fit_params,\n",
    "            max_evals=2,\n",
    "        )\n",
    "        \n",
    "        fnvals = [(t['result']['metrics']) for t in trials.trials]\n",
    "        params = max(fnvals, key = lambda x: x['Accuracy']) # (accuracy, auc, precision)\n",
    "        if params['Accuracy'] > best_acc:\n",
    "            best_metrics = params\n",
    "            best_params = curr_best_params\n",
    "            trials = trials\n",
    "            \n",
    "    return best_params, best_metrics, trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def outer_cv(X, y, results_df, record, n_splits=10):\n",
    "    df = results_df.copy()\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    for index, (tr_ind, tset_ind) in enumerate(kf.split(X, y)):\n",
    "        print(\"Starting {i} fold out of {n} outher folds\".format(i=index,n=n_splits))\n",
    "        \n",
    "        X_train = X.iloc[tr_ind].copy()\n",
    "        y_train = y.iloc[tr_ind].copy()\n",
    "        \n",
    "        X_test = X.iloc[tset_ind].copy()\n",
    "        y_test = y.iloc[tset_ind].copy()\n",
    "        best_params, best_metrics, trials = inner_cv_hyperopt(X_train,y_train)\n",
    "        info = {'CV_fold':index,\n",
    "                'HP_vals':{k: best_params[k] for k in list(best_params)[:3]},\n",
    "                **best_metrics\n",
    "               }\n",
    "        record.update(info)\n",
    "        df = df.append(record, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already fitted dataset: abalon\n",
      "Skipping already fitted dataset: acute-inflammation\n",
      "Fitting on acute-nephritis dataset\n",
      "Starting 0 fold out of 10 outher folds\n",
      "Starting 0 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.86trial/s, best loss: -1.0]\n",
      "Starting 1 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.31trial/s, best loss: -1.0]\n",
      "Starting 2 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.88trial/s, best loss: -1.0]\n",
      "Starting 1 fold out of 10 outher folds\n",
      "Starting 0 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.57trial/s, best loss: -1.0]\n",
      "Starting 1 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.80trial/s, best loss: -1.0]\n",
      "Starting 2 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.87trial/s, best loss: -1.0]\n",
      "Starting 2 fold out of 10 outher folds\n",
      "Starting 0 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.51trial/s, best loss: -1.0]\n",
      "Starting 1 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.85trial/s, best loss: -1.0]\n",
      "Starting 2 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.41trial/s, best loss: -1.0]\n",
      "Starting 3 fold out of 10 outher folds\n",
      "Starting 0 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.87trial/s, best loss: -1.0]\n",
      "Starting 1 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.80trial/s, best loss: -1.0]\n",
      "Starting 2 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.24trial/s, best loss: -1.0]\n",
      "Starting 4 fold out of 10 outher folds\n",
      "Starting 0 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.39trial/s, best loss: -1.0]\n",
      "Starting 1 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.38trial/s, best loss: -1.0]\n",
      "Starting 2 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.32trial/s, best loss: -1.0]\n",
      "Starting 5 fold out of 10 outher folds\n",
      "Starting 0 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.62trial/s, best loss: -1.0]\n",
      "Starting 1 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.49trial/s, best loss: -1.0]\n",
      "Starting 2 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.60trial/s, best loss: -1.0]\n",
      "Starting 6 fold out of 10 outher folds\n",
      "Starting 0 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.78trial/s, best loss: -1.0]\n",
      "Starting 1 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.89trial/s, best loss: -1.0]\n",
      "Starting 2 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.52trial/s, best loss: -1.0]\n",
      "Starting 7 fold out of 10 outher folds\n",
      "Starting 0 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.93trial/s, best loss: -1.0]\n",
      "Starting 1 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.04trial/s, best loss: -1.0]\n",
      "Starting 2 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.89trial/s, best loss: -1.0]\n",
      "Starting 8 fold out of 10 outher folds\n",
      "Starting 0 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.95trial/s, best loss: -1.0]\n",
      "Starting 1 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.70trial/s, best loss: -1.0]\n",
      "Starting 2 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.01trial/s, best loss: -1.0]\n",
      "Starting 9 fold out of 10 outher folds\n",
      "Starting 0 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.70trial/s, best loss: -1.0]\n",
      "Starting 1 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.01trial/s, best loss: -1.0]\n",
      "Starting 2 fold out of 3 inner folds\n",
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.28trial/s, best loss: -1.0]\n",
      "Fitting on analcatdata_asbestos dataset\n",
      "Starting 0 fold out of 10 outher folds\n",
      "Starting 0 fold out of 3 inner folds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-320-13636b96995b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mresults_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mouter_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mresults_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRESULTS_FILENAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-319-4624556fab9f>\u001b[0m in \u001b[0;36mouter_cv\u001b[1;34m(X, y, results_df, record, n_splits)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtset_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtset_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mbest_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner_cv_hyperopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         info = {'CV_fold':index,\n\u001b[0;32m     15\u001b[0m                 \u001b[1;34m'HP_vals'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-318-f106996bc43c>\u001b[0m in \u001b[0;36minner_cv_hyperopt\u001b[1;34m(X, y, n_splits)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mparameter_space\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         )\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-317-58f5d674e0e1>\u001b[0m in \u001b[0;36mfind_best_params\u001b[1;34m(X_train, y_train, X_test, y_test, model, const_params, parameter_space, fit_params, max_evals, cv_splitter, cv_scoring)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     objective = HyperoptObjective(X_train, y_train, X_test, y_test, model, const_params, \n\u001b[1;32m---> 15\u001b[1;33m                                   fit_params)\n\u001b[0m\u001b[0;32m     16\u001b[0m     '''\n\u001b[0;32m     17\u001b[0m     \u001b[0mHyperOpt\u001b[0m \u001b[0mTrials\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mstores\u001b[0m \u001b[0mdetails\u001b[0m \u001b[0mof\u001b[0m \u001b[0mevery\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-316-6524017bf674>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, X_train, y_train, X_test, y_test, model, const_params, fit_params)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'object'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "results_df = pd.DataFrame(columns=['Dataset','Algorithm','CV_fold','HP_vals','Accuracy','TPR','FPR','Precision','AUC','PR-Curve','Training_time','Inference_time'])\n",
    "RESULTS_FILENAME = \"catboost-results.csv\"\n",
    "results_file = Path(RESULTS_FILENAME)\n",
    "if results_file.is_file():\n",
    "    results_df = pd.read_csv(RESULTS_FILENAME)\n",
    "\n",
    "for cls_dataset in glob.glob('classification_datasets/*.csv'): \n",
    "    dataset_name = Path(cls_dataset).stem\n",
    "    if dataset_name in results_df.Dataset.unique():\n",
    "        print(\"Skipping already fitted dataset: {f}\".format(f=dataset_name))\n",
    "        continue\n",
    "    print(\"Fitting on {f} dataset\".format(f=dataset_name))\n",
    "    record = {'Dataset':dataset_name,'Algorithm':'Catboost'}\n",
    "    df = pd.read_csv(cls_dataset)\n",
    "    X = df[df.columns[:-1]]\n",
    "    y = df[df.columns[-1]]\n",
    "    results_df = outer_cv(X,y, results_df,record)\n",
    "    results_df.to_csv(RESULTS_FILENAME,index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
